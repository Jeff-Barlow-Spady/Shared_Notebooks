{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_profiling in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (3.6.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lektor 3.3.10 requires Babel, which is not installed.\n",
      "lektor 3.3.10 requires Flask, which is not installed.\n",
      "lektor 3.3.10 requires mistune<2,>=0.7.0, which is not installed.\n",
      "lektor 3.3.10 requires python-slugify, which is not installed.\n",
      "lektor 3.3.10 requires Werkzeug<2.4, which is not installed.\n",
      "altair 5.0.1 requires jsonschema>=3.0, which is not installed.\n",
      "altair 5.0.1 requires toolz, which is not installed.\n",
      "autoviz 0.1.730 requires bokeh~=2.4.2, which is not installed.\n",
      "autoviz 0.1.730 requires fsspec>=0.8.3, which is not installed.\n",
      "autoviz 0.1.730 requires jupyter, which is not installed.\n",
      "autoviz 0.1.730 requires nltk, which is not installed.\n",
      "autoviz 0.1.730 requires panel>=0.12.6, which is not installed.\n",
      "autoviz 0.1.730 requires scikit-learn, which is not installed.\n",
      "bqplot 0.12.40 requires ipywidgets<9,>=7.5.0, which is not installed.\n",
      "camelot-py 0.9.0 requires chardet>=3.0.4, which is not installed.\n",
      "camelot-py 0.9.0 requires openpyxl>=2.5.8, which is not installed.\n",
      "category-encoders 2.6.1 requires scikit-learn>=0.20.0, which is not installed.\n",
      "dash 2.11.1 requires Flask<2.3.0,>=1.0.4, which is not installed.\n",
      "dash 2.11.1 requires Werkzeug<2.3.0, which is not installed.\n",
      "databricks-cli 0.17.7 requires pyjwt>=1.7.0, which is not installed.\n",
      "databricks-cli 0.17.7 requires tabulate>=0.7.7, which is not installed.\n",
      "dataprep 0.4.5 requires bokeh<3,>=2, which is not installed.\n",
      "dataprep 0.4.5 requires dask[array,dataframe,delayed]>=2022.3.0, which is not installed.\n",
      "dataprep 0.4.5 requires flask<3,>=2, which is not installed.\n",
      "dataprep 0.4.5 requires ipywidgets<8.0,>=7.5, which is not installed.\n",
      "dataprep 0.4.5 requires nltk<4.0.0,>=3.6.7, which is not installed.\n",
      "docker 6.1.3 requires websocket-client>=0.32.0, which is not installed.\n",
      "dtale 3.2.0 requires beautifulsoup4; python_version > \"3.0\", which is not installed.\n",
      "dtale 3.2.0 requires et-xmlfile; python_version >= \"3.6\", which is not installed.\n",
      "dtale 3.2.0 requires Flask<2.3; python_version >= \"3.7\", which is not installed.\n",
      "dtale 3.2.0 requires future>=0.14.0, which is not installed.\n",
      "dtale 3.2.0 requires itsdangerous; python_version >= \"3.7\", which is not installed.\n",
      "dtale 3.2.0 requires lz4; python_version > \"3.6\", which is not installed.\n",
      "dtale 3.2.0 requires openpyxl!=3.2.0b1; python_version >= \"3.0\", which is not installed.\n",
      "dtale 3.2.0 requires scikit-learn; python_version > \"3.7\", which is not installed.\n",
      "dtale 3.2.0 requires werkzeug<2.3; python_version >= \"3.7\", which is not installed.\n",
      "dtale 3.2.0 requires xarray; python_version >= \"3.0\", which is not installed.\n",
      "flask-ngrok 0.0.25 requires Flask>=0.8, which is not installed.\n",
      "ghostwriter 0.1 requires Cryptography, which is not installed.\n",
      "ghostwriter 0.1 requires itsdangerous, which is not installed.\n",
      "ghostwriter 0.1 requires PyQt5, which is not installed.\n",
      "ghostwriter 0.1 requires PyQt5-sip, which is not installed.\n",
      "ghostwriter 0.1 requires PySocks, which is not installed.\n",
      "ghostwriter 0.1 requires sip, which is not installed.\n",
      "ghostwriter 0.1 requires Werkzeug, which is not installed.\n",
      "holoviews 1.14.9 requires colorcet, which is not installed.\n",
      "holoviews 1.14.9 requires panel>=0.8.0, which is not installed.\n",
      "holoviews 1.14.9 requires param<2.0,>=1.9.3, which is not installed.\n",
      "holoviews 1.14.9 requires pyviz-comms>=0.7.4, which is not installed.\n",
      "hvplot 0.7.3 requires bokeh>=1.0.0, which is not installed.\n",
      "hvplot 0.7.3 requires colorcet>=2, which is not installed.\n",
      "ipydatawidgets 4.3.5 requires ipywidgets>=7.0.0, which is not installed.\n",
      "ipyvolume 0.6.3 requires ipywidgets>=7.0.0, which is not installed.\n",
      "langchain 0.0.232 requires numexpr<3.0.0,>=2.8.4, which is not installed.\n",
      "mitosheet 0.1.508 requires chardet>=3.0.4, which is not installed.\n",
      "mitosheet 0.1.508 requires jupyterlab~=3.0, which is not installed.\n",
      "mitosheet 0.1.508 requires openpyxl, which is not installed.\n",
      "mlflow 2.4.2 requires cloudpickle<3, which is not installed.\n",
      "mlflow 2.4.2 requires entrypoints<1, which is not installed.\n",
      "mlflow 2.4.2 requires Flask<3, which is not installed.\n",
      "mlflow 2.4.2 requires markdown<4,>=3.3, which is not installed.\n",
      "mlflow 2.4.2 requires scikit-learn<2, which is not installed.\n",
      "pandas-dq 1.28 requires scikit-learn>=0.24.2, which is not installed.\n",
      "pdfminer-six 20221105 requires cryptography>=36.0.0, which is not installed.\n",
      "pmdarima 2.0.3 requires scikit-learn>=0.22, which is not installed.\n",
      "pycaret 3.0.4 requires cloudpickle, which is not installed.\n",
      "pycaret 3.0.4 requires imbalanced-learn>=0.8.1, which is not installed.\n",
      "pycaret 3.0.4 requires ipywidgets>=7.6.5, which is not installed.\n",
      "pycaret 3.0.4 requires nbformat>=4.2.0, which is not installed.\n",
      "pycaret 3.0.4 requires numba>=0.55.0, which is not installed.\n",
      "pycaret 3.0.4 requires scikit-learn<1.3.0,>=1.0, which is not installed.\n",
      "pygwalker 0.3.7 requires arrow, which is not installed.\n",
      "pygwalker 0.3.7 requires ipywidgets, which is not installed.\n",
      "pyod 1.1.0 requires numba>=0.51, which is not installed.\n",
      "pyod 1.1.0 requires scikit-learn>=0.20.0, which is not installed.\n",
      "pythreejs 2.4.2 requires ipywidgets>=7.2.1, which is not installed.\n",
      "scikit-plot 0.3.7 requires scikit-learn>=0.18, which is not installed.\n",
      "sktime 0.20.0 requires scikit-learn<1.3.0,>=0.24.0, which is not installed.\n",
      "streamlit 1.24.1 requires toml<2, which is not installed.\n",
      "tbats 1.1.3 requires scikit-learn, which is not installed.\n",
      "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
      "databricks-cli 0.17.7 requires urllib3<2.0.0,>=1.26.7, but you have urllib3 2.0.7 which is incompatible.\n",
      "langchain 0.0.232 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.3.24 which is incompatible.\n",
      "mlflow 2.4.2 requires sqlalchemy<3,>=1.4.0, but you have sqlalchemy 1.3.24 which is incompatible.\n",
      "streamlit 1.24.1 requires pillow<10,>=6.2.0, but you have pillow 10.1.0 which is incompatible.\n",
      "streamsync 0.2.8 requires pydantic<3,>=2.1.1, but you have pydantic 1.10.13 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: ydata-profiling in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from pandas_profiling) (4.5.1)\n",
      "Requirement already satisfied: scipy<1.12,>=1.4.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (1.9.3)\n",
      "Requirement already satisfied: pandas!=1.4.0,<2.1,>1.1 in c:\\users\\toast\\.conda\\envs\\.venv\\lib\\site-packages (from ydata-profiling->pandas_profiling) (1.5.3)\n",
      "Requirement already satisfied: matplotlib<4,>=3.2 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (3.6.3)\n",
      "Collecting pydantic<2,>=1.8.1 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "                                              0.0/2.1 MB 163.8 kB/s eta 0:00:13\n",
      "                                              0.0/2.1 MB 196.9 kB/s eta 0:00:11\n",
      "     --                                       0.1/2.1 MB 561.1 kB/s eta 0:00:04\n",
      "     -----                                    0.3/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     -------------                            0.7/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------                1.3/2.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.9/2.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting PyYAML<6.1,>=5.0.0 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "                                              0.0/145.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 145.3/145.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (3.0.3)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.5 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (0.7.5)\n",
      "Collecting numpy<1.24,>=1.16.0 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "                                              0.0/14.6 MB ? eta -:--:--\n",
      "     -                                        0.6/14.6 MB 17.5 MB/s eta 0:00:01\n",
      "     --                                       1.0/14.6 MB 12.7 MB/s eta 0:00:02\n",
      "     ----                                     1.5/14.6 MB 11.7 MB/s eta 0:00:02\n",
      "     ----                                     1.8/14.6 MB 10.3 MB/s eta 0:00:02\n",
      "     ------                                   2.3/14.6 MB 10.7 MB/s eta 0:00:02\n",
      "     -------                                  2.9/14.6 MB 10.8 MB/s eta 0:00:02\n",
      "     --------                                 3.3/14.6 MB 11.0 MB/s eta 0:00:02\n",
      "     ----------                               4.0/14.6 MB 11.1 MB/s eta 0:00:01\n",
      "     -----------                              4.3/14.6 MB 10.5 MB/s eta 0:00:01\n",
      "     -------------                            4.9/14.6 MB 10.7 MB/s eta 0:00:01\n",
      "     --------------                           5.3/14.6 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------                         6.0/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------                       6.6/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     -------------------                      7.3/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------                   8.1/14.6 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------------------                 8.8/14.6 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------------------               9.6/14.6 MB 11.0 MB/s eta 0:00:01\n",
      "     ----------------------------            10.5/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     -----------------------------           11.0/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------------          11.6/14.6 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------        12.2/14.6 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------------------      13.1/14.6 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------    13.7/14.6 MB 11.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.3/14.6 MB 11.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 10.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: htmlmin==0.1.12 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (0.12.3)\n",
      "Collecting requests<3,>=2.24.0 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "                                              0.0/62.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.6/62.6 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting tqdm<5,>=4.48.2 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "                                              0.0/78.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting seaborn<0.13,>=0.10.1 (from ydata-profiling->pandas_profiling)\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "                                              0.0/293.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 293.3/293.3 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (1.10)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (0.14.0)\n",
      "Requirement already satisfied: typeguard<3,>=2.13.2 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (2.13.3)\n",
      "Requirement already satisfied: imagehash==4.3.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (1.9.2)\n",
      "Requirement already satisfied: dacite>=1.8 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from ydata-profiling->pandas_profiling) (1.8.1)\n",
      "Collecting PyWavelets (from imagehash==4.3.1->ydata-profiling->pandas_profiling)\n",
      "  Downloading PyWavelets-1.4.1-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "                                              0.0/4.2 MB ? eta -:--:--\n",
      "     -------                                  0.8/4.2 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------                            1.4/4.2 MB 14.8 MB/s eta 0:00:01\n",
      "     ----------------                         1.8/4.2 MB 13.9 MB/s eta 0:00:01\n",
      "     -------------------------                2.6/4.2 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------------------------           3.2/4.2 MB 11.4 MB/s eta 0:00:01\n",
      "     -------------------------------------    3.9/4.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.2 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.2/4.2 MB 9.5 MB/s eta 0:00:00\n",
      "Collecting pillow (from imagehash==4.3.1->ydata-profiling->pandas_profiling)\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     -------                                  0.5/2.6 MB 16.2 MB/s eta 0:00:01\n",
      "     ---------------                          1.0/2.6 MB 12.6 MB/s eta 0:00:01\n",
      "     --------------------                     1.3/2.6 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------                   1.4/2.6 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------------         2.1/2.6 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 9.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 8.8 MB/s eta 0:00:00\n",
      "Collecting attrs>=19.3.0 (from visions[type_image_path]==0.7.5->ydata-profiling->pandas_profiling)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "                                              0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Collecting networkx>=2.4 (from visions[type_image_path]==0.7.5->ydata-profiling->pandas_profiling)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------                          0.6/1.6 MB ? eta -:--:--\n",
      "     -----------------                        0.7/1.6 MB 8.9 MB/s eta 0:00:01\n",
      "     -----------------                        0.7/1.6 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------------           1.3/1.6 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas_profiling) (0.2.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas_profiling)\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling)\n",
      "  Downloading contourpy-1.1.1-cp310-cp310-win_amd64.whl (477 kB)\n",
      "                                              0.0/478.0 kB ? eta -:--:--\n",
      "     ----------------------------------    440.3/478.0 kB 26.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 478.0/478.0 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10 (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling)\n",
      "  Downloading fonttools-4.43.1-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "     -------                                  0.4/2.1 MB 8.7 MB/s eta 0:00:01\n",
      "     -----------------                        0.9/2.1 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------------------               1.4/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "     --------------------------------         1.8/2.1 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "                                              0.0/56.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.1/56.1 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling) (23.1)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "                                              0.0/103.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 103.1/103.1 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.2->ydata-profiling->pandas_profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\toast\\.conda\\envs\\.venv\\lib\\site-packages (from pandas!=1.4.0,<2.1,>1.1->ydata-profiling->pandas_profiling) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas_profiling) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<2,>=1.8.1->ydata-profiling->pandas_profiling) (4.7.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.24.0->ydata-profiling->pandas_profiling)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "                                              0.0/100.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 100.3/100.3 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toast\\.conda\\envs\\.venv\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling->pandas_profiling) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.24.0->ydata-profiling->pandas_profiling)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "                                              0.0/124.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 124.2/124.2 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.24.0->ydata-profiling->pandas_profiling)\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "                                              0.0/158.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 158.3/158.3 kB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\toast\\.conda\\envs\\.venv\\lib\\site-packages (from statsmodels<1,>=0.13.2->ydata-profiling->pandas_profiling) (0.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5,>=4.48.2->ydata-profiling->pandas_profiling) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling->pandas_profiling) (1.16.0)\n",
      "Installing collected packages: urllib3, tqdm, PyYAML, pyparsing, pydantic, pillow, numpy, networkx, MarkupSafe, kiwisolver, fonttools, cycler, charset-normalizer, certifi, attrs, requests, PyWavelets, contourpy, seaborn\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.4.2\n",
      "    Uninstalling pydantic-2.4.2:\n",
      "      Successfully uninstalled pydantic-2.4.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed MarkupSafe-2.1.3 PyWavelets-1.4.1 PyYAML-6.0.1 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.2 contourpy-1.1.1 cycler-0.12.1 fonttools-4.43.1 kiwisolver-1.4.5 networkx-3.2.1 numpy-1.23.5 pillow-10.1.0 pydantic-1.10.13 pyparsing-3.1.1 requests-2.31.0 seaborn-0.12.2 tqdm-4.66.1 urllib3-2.0.7\n",
      "Requirement already satisfied: pydantic in c:\\users\\toast\\.conda\\envs\\.venv\\lib\\site-packages (1.10.13)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\toast\\appdata\\roaming\\python\\python310\\site-packages (from pydantic) (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toast\\.conda\\envs\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\toast\\AppData\\Local\\Temp\\ipykernel_18256\\3318085536.py:8: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  import pandas_profiling\n"
     ]
    }
   ],
   "source": [
    "#!pip install polars\n",
    "%pip install pandas_profiling\n",
    "%pip install pydantic\n",
    "import os\n",
    "import json\n",
    "import polars as pl \n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import geopandas as gpd\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining file: Neighbourhoods_and_Wards_20231017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset:  11%|█         | 1/9 [00:00<00:01,  5.98it/s, Describe variable:Neighbourhood Number]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 17/17 [00:10<00:00,  1.62it/s, Completed]                                                   \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.22s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 85.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection complete.\n",
      "-----\n",
      "Examining file: Root_for_Trees_Inventory_20231022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 46/46 [00:14<00:00,  3.10it/s, Completed]                                         \n",
      "Generate report structure: 100%|██████████| 1/1 [00:12<00:00, 12.94s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection complete.\n",
      "-----\n",
      "Examining file: Tree_Insects___Other_Pests_20231017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 22/22 [00:04<00:00,  5.39it/s, Completed]                                     \n",
      "Generate report structure: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection complete.\n",
      "-----\n",
      "Examining file: Vacant_Land_-_Industrial_20231017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 49/49 [00:14<00:00,  3.31it/s, Completed]                                         \n",
      "Generate report structure: 100%|██████████| 1/1 [00:12<00:00, 12.81s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 32.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection complete.\n",
      "-----\n",
      "Inspection results saved to C:\\Users\\toast\\Downloads\\Climate_Data\\inspection_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Installs the Polars library for working with tabular data\n",
    "# Imports libraries for working with different data file types and generating data profiling reports\n",
    "# Loops through all files in the input directory\n",
    "# Determines file type and reads into appropriate DataFrame \n",
    "# Generates a ProfileReport for key info like data types, missing values\n",
    "# Extracts schema info, missing data stats, and other useful info\n",
    "# Saves results to a JSON file for later analysis\n",
    "\n",
    "\n",
    "# Gather schema information\n",
    "inspection_results = []\n",
    "\n",
    "directory = r\"C:\\Users\\toast\\Downloads\\Climate_Data\\Tabular\\test\"\n",
    "output_folder = r\"C:\\Users\\toast\\Downloads\\Climate_Data\\Tabular\\test\\profiles\"\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith((\".csv\", \".xlsx\", \".geojson\", \".shp\")):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"Examining file: {file}\")\n",
    "        \n",
    "        # Determine the file format and use appropriate library\n",
    "        if file.endswith((\".csv\", \".xlsx\")):\n",
    "            try:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                else:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                \n",
    "                # Generate a data profiling report\n",
    "                profile = ProfileReport(df)\n",
    "                # Save the profile report with the correct file extension\n",
    "                output_file_name = f\"profile_{os.path.splitext(file)[0]}.html\"  # Corrected the extension\n",
    "                profile.to_file(os.path.join(output_folder, output_file_name))\n",
    "                \n",
    "                # Gather schema information\n",
    "                schema_info = df.dtypes.to_dict()\n",
    "                \n",
    "                # Check for missing data\n",
    "                missing_data = df.isnull().sum().to_dict()\n",
    "                \n",
    "                # Extract useful information (customize as needed)\n",
    "                useful_info = {\n",
    "                    \"Total Rows\": len(df),\n",
    "                    \"Total Columns\": len(df.columns),\n",
    "                    \"Column Names\": df.columns.tolist(),\n",
    "                    \"Profile Report\": output_file_name                 ,\n",
    "                    \"Data Types\": schema_info,\n",
    "                    \"Missing Data\": missing_data,\n",
    "\n",
    "                    # Add more information as needed\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Handle any errors when reading the file\n",
    "                schema_info = {}\n",
    "                missing_data = {}\n",
    "                useful_info = {\"Error\": str(e)}\n",
    "        \n",
    "        elif file.endswith((\".geojson\", \".shp\")):\n",
    "            try:\n",
    "                if file.endswith(\".geojson\"):\n",
    "                    df = gpd.read_file(file_path)\n",
    "                else:\n",
    "                    df = gpd.read_file(file_path)\n",
    "                \n",
    "                # Gather schema information\n",
    "                schema_info = dict(df.dtypes)\n",
    "                \n",
    "                # Check for missing data\n",
    "                missing_data = df.isnull().sum().to_dict()\n",
    "                \n",
    "                # Extract useful information (customize as needed)\n",
    "                useful_info = {\n",
    "                    \"Total Rows\": len(df),\n",
    "                    \"Total Columns\": len(df.columns),\n",
    "                    \"Column Names\": df.columns.tolist(),\n",
    "                    \"Data Types\": schema_info,\n",
    "                    \"Missing Data\": missing_data,\n",
    "\n",
    "                    # Add more information as needed\n",
    "                }\n",
    "            except Exception as e:\n",
    "                # Handle any errors when reading the file\n",
    "                schema_info = {}\n",
    "                missing_data = {}\n",
    "                useful_info = {\"Error\": str(e)}\n",
    "        \n",
    "        # Store the inspection results in a dictionary\n",
    "        result = {\n",
    "            \"File Name\": file,\n",
    "            \"Missing Data\": missing_data,\n",
    "            \"Schema Info\": schema_info,\n",
    "            \"Useful Info\": useful_info,\n",
    "        }\n",
    "        \n",
    "        # Append the result to the list\n",
    "        inspection_results.append(result)\n",
    "        print(\"Inspection complete.\")\n",
    "        print(\"-----\")\n",
    "\n",
    "        # Save the profile report to disk\n",
    "        if \"Profile Report\" in useful_info:\n",
    "            profile_path = useful_info[\"Profile Report\"]\n",
    "            if os.path.exists(profile_path):\n",
    "                os.remove(profile_path)\n",
    "            if os.path.exists(f\"profile_{file}.html\"):\n",
    "                if os.path.exists(profile_path):\n",
    "                    os.remove(profile_path)\n",
    "                os.rename(f\"profile_{file}.html\", profile_path)\n",
    "\n",
    "# Save the inspection results to a JSON file\n",
    "output_file = r\"C:\\Users\\toast\\Downloads\\Climate_Data\\inspection_results.json\"\n",
    "\n",
    "def convert_dtype(obj):\n",
    "    if isinstance(obj, pd.Timestamp):\n",
    "        return obj.isoformat()\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(inspection_results, json_file, default=convert_dtype)\n",
    "\n",
    "print(f\"Inspection results saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(inspection_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(r\"C:\\Users\\toast\\Downloads\\Climate_Data\\inspection_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
